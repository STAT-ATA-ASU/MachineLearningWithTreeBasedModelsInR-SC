[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning with Tree-Based Models in R",
    "section": "",
    "text": "Preface\nThis material is from the DataCamp course Machine Learning with Tree-Based Models in R by Sandro Raabe.\nCourse Description: Tree-based machine learning models can reveal complex non-linear relationships in data and often dominate machine learning competitions. In this course, you’ll use the tidymodels package to explore and build different tree-based models—from simple decision trees to complex random forests. You’ll also learn to use boosted trees, a powerful machine learning technique that uses ensemble learning to build high-performing predictive models. Along the way, you’ll work with health and credit risk data to predict the incidence of diabetes and customer churn.\nReminder to self: each *.qmd file contains one and only one chapter, and a chapter is defined by the first-level heading #."
  },
  {
    "objectID": "01-MLTB.html#welcome-to-the-course---video",
    "href": "01-MLTB.html#welcome-to-the-course---video",
    "title": "1  Classification Trees",
    "section": "Welcome to the course! - (video)",
    "text": "Welcome to the course! - (video)"
  },
  {
    "objectID": "01-MLTB.html#why-tree-based-methods",
    "href": "01-MLTB.html#why-tree-based-methods",
    "title": "1  Classification Trees",
    "section": "1.1 Why tree-based methods?",
    "text": "1.1 Why tree-based methods?\nTree-based models are one class of methods used in machine learning. They are superior in many ways, but also have their drawbacks.\nWhich of these statements are true and which are false?"
  },
  {
    "objectID": "01-MLTB.html#specify-that-tree",
    "href": "01-MLTB.html#specify-that-tree",
    "title": "1  Classification Trees",
    "section": "1.2 Specify that tree",
    "text": "1.2 Specify that tree\nIn order to build models and use them to solve real-world problems, you first need to lay the foundations of your model by creating a model specification. This is the very first step in every machine learning pipeline that you will ever build.\nYou are going to load the relevant packages and design the specification for your classification tree in just a few steps.\nA magical moment, enjoy!\n\nInstructions\n\nLoad the tidymodels package.\n\n\nlibrary(tidymodels)\n\n\nPick a model class for decision trees, save it as tree_spec, and print it.\n\n\n# Pick a model class\ntree_spec &lt;- decision_tree() \n\n# Print the result\ntree_spec\n\nDecision Tree Model Specification (unknown mode)\n\nComputational engine: rpart \n\n\n\nSet the engine to \"rpart\" and print the result.\n\n\n# Pick a model class\ntree_spec &lt;- decision_tree() |&gt;  \n  # Set the engine\n  set_engine(\"rpart\")\n\n# Print the result\ntree_spec\n\nDecision Tree Model Specification (unknown mode)\n\nComputational engine: rpart \n\n\n\nSet the mode to \"classification\" and print the result.\n\n\n# Pick a model class\ntree_spec &lt;- decision_tree() |&gt;  \n  # Set the engine\n  set_engine(\"rpart\") |&gt;  \n  # Set the mode\n  set_mode(\"classification\")\n\n# Print the result\ntree_spec\n\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\n\n\n\n\n\n\n\nNote\n\n\n\nYou created a decision tree model class, used an rpart engine, and set the mode to \"classification\". Remember, you will need to perform similar steps every time you design a new model. Come back anytime if you need a reminder!"
  },
  {
    "objectID": "01-MLTB.html#train-that-model",
    "href": "01-MLTB.html#train-that-model",
    "title": "1  Classification Trees",
    "section": "1.3 Train that model",
    "text": "1.3 Train that model\nA model specification is a good start, just like the canvas for a painter. But just as a painter needs color, the specification needs data. Only the final model is able to make predictions:\nModel specification + data = model\nIn this exercise, you will train a decision tree that models the risk of diabetes using health variables as predictors. The response variable, outcome, indicates whether the patient has diabetes or not, which means this is a binary classification problem (there are just two classes). The dataset also contains health variables of patients like blood_pressure, age, and bmi.\nFor the rest of the course, the tidymodels package will always be pre-loaded. In this exercise, the diabetes dataset is also available in your workspace.\n\ndiabetes &lt;- read_csv(\"./data/diabetes_tibble.csv\")\n# Change character outcome to a factor\ndiabetes &lt;- diabetes |&gt;  mutate_if(is.character, as.factor)\n\n\nInstructions\n\nCreate tree_spec, a specification for a decision tree with an rpart engine.\n\n\n# Create the specification\ntree_spec &lt;- decision_tree() |&gt;  \n  set_engine(\"rpart\") |&gt;  \n  set_mode(\"classification\")\n\n\nTrain a model tree_model_bmi, where the outcome depends only on the bmi predictor by fitting the diabetes dataset to the specification.\n\n\n# Train the model\ntree_model_bmi &lt;- tree_spec |&gt;  \n  fit(outcome ~ bmi, data = diabetes)\n\n\nPrint the model to the console.\n\n\n# Print the model\ntree_model_bmi\n\nparsnip model object\n\nn= 768 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 768 268 no (0.6510417 0.3489583)  \n  2) bmi&lt; 29.85 291  47 no (0.8384880 0.1615120) *\n  3) bmi&gt;=29.85 477 221 no (0.5366876 0.4633124)  \n    6) bmi&lt; 40.85 392 168 no (0.5714286 0.4285714) *\n    7) bmi&gt;=40.85 85  32 yes (0.3764706 0.6235294) *\n\n\n\nGraph the model with rpart.plot()\n\n\nrpart.plot::rpart.plot(tree_model_bmi$fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou have defined your model with decision_tree() and trained it to predict outcome using bmi like a professional coach! Printing the model displays useful information, such as the training time, the model formula used during training, and the node details. Remember, to fit a model to data is just a different phrase for training it. Don’t worry about the precise output too much, you’ll cover that later!"
  },
  {
    "objectID": "01-MLTB.html#how-to-grow-your-tree---video",
    "href": "01-MLTB.html#how-to-grow-your-tree---video",
    "title": "1  Classification Trees",
    "section": "How to grow your tree - (video)",
    "text": "How to grow your tree - (video)"
  },
  {
    "objectID": "01-MLTB.html#traintest-split",
    "href": "01-MLTB.html#traintest-split",
    "title": "1  Classification Trees",
    "section": "1.4 Train/test split",
    "text": "1.4 Train/test split\nIn order to test your models, you need to build and test the model on two different parts of the data - otherwise, it’s like cheating on an exam (as you already know the answers).\nThe data split is an integral part of the modeling process. You will dive into this by splitting the diabetes data and confirming the split proportions.\nThe diabetes data from the last exercise is pre-loaded in your workspace.\n\nInstructions\n\nSplit the diabetes tibble into diabetes_split, a split of 80% training and 20% test data.\n\n\nset.seed(123)\n# Create the split\ndiabetes_split &lt;- initial_split(diabetes, prop = 0.80)\n\n\nPrint the resulting object.\n\n\n# Print the data split\ndiabetes_split\n\n&lt;Training/Testing/Total&gt;\n&lt;614/154/768&gt;\n\n\n\nExtract the training and test sets and save them as diabetes_train and diabetes_test.\n\n\n# Extract the training and test set\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test  &lt;- testing(diabetes_split)\n\n\nVerify the correct row proportion in both datasets compared to the diabetes tibble.\n\n\n# Verify the proportions of both sets\nround(nrow(diabetes_train) / nrow(diabetes), 2) == 0.80\n\n[1] TRUE\n\nround(nrow(diabetes_test) / nrow(diabetes), 2) == 0.20\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing training() and testing() after the split ensures that you save your working datasets."
  },
  {
    "objectID": "01-MLTB.html#avoiding-class-imbalances",
    "href": "01-MLTB.html#avoiding-class-imbalances",
    "title": "1  Classification Trees",
    "section": "1.5 Avoiding class imbalances",
    "text": "1.5 Avoiding class imbalances\nSome data contains very imbalanced outcomes - like a rare disease dataset. When splitting randomly, you might end up with a very unfortunate split. Imagine all the rare observations are in the test and none in the training set. That would ruin your whole training process!\nFortunately, the initial_split() function provides a remedy. You are going to observe and solve these so-called class imbalances in this exercise.\nThere is already code provided to create a split object diabetes_split with a 75% training and 25% test split.\n\n# Preparation\nset.seed(9888)\ndiabetes_split &lt;- initial_split(diabetes, prop = 0.75)\n\n\nInstructions\n\nCount the proportion of \"yes\" outcomes in the training and test sets of diabetes_split.\n\n\n# Proportion of 'yes' outcomes in the training data\ncounts_train &lt;- table(training(diabetes_split)$outcome)\nprop_yes_train &lt;- counts_train[\"yes\"] / sum(counts_train)\n\n# Proportion of 'yes' outcomes in the test data\ncounts_test &lt;- table(testing(diabetes_split)$outcome)\nprop_yes_test &lt;- counts_test[\"yes\"] / sum(counts_test)\n\npaste(\"Proportion of positive outcomes in training set:\", round(prop_yes_train, 2))\n\n[1] \"Proportion of positive outcomes in training set: 0.31\"\n\npaste(\"Proportion of positive outcomes in test set:\", round(prop_yes_test, 2))\n\n[1] \"Proportion of positive outcomes in test set: 0.46\"\n\n\n\nRedesign diabetes_split using the same training/testing proportion, but with the outcome variable being equally distributed in both sets. Count the proportion of yes outcomes in both datasets.\n\n\nset.seed(123)\n# Create a split with a constant outcome distribution\ndiabetes_split &lt;- initial_split(diabetes, strata = outcome)\n\n# Proportion of 'yes' outcomes in the training data\ncounts_train &lt;- table(training(diabetes_split)$outcome)\nprop_yes_train &lt;- counts_train[\"yes\"] / sum(counts_train)\n\n# Proportion of 'yes' outcomes in the test data\ncounts_test &lt;- table(testing(diabetes_split)$outcome)\nprop_yes_test &lt;- counts_test['yes'] / sum(counts_test)\n\npaste(\"Proportion of positive outcomes in training set:\", round(prop_yes_train, 2))\n\n[1] \"Proportion of positive outcomes in training set: 0.35\"\n\npaste(\"Proportion of positive outcomes in test set:\", round(prop_yes_test, 2))\n\n[1] \"Proportion of positive outcomes in test set: 0.35\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nImpressive - from 31% vs. 46% positive outcomes to 35% in both sets. This was a tough one, but now you know how simple it is to avoid class imbalances! This is even more important in a large dataset with a very imbalanced target variable."
  },
  {
    "objectID": "01-MLTB.html#from-zero-to-hero",
    "href": "01-MLTB.html#from-zero-to-hero",
    "title": "1  Classification Trees",
    "section": "1.6 From zero to hero",
    "text": "1.6 From zero to hero\nYou mastered the skills of creating a model specification and splitting the data into training and test sets. You also know how to avoid class imbalances in the split. It’s now time to combine what you learned in the preceding lesson and build your model using only the training set!\nYou are going to build a proper machine learning pipeline. This is comprised of creating a model specification, splitting your data into training and test sets, and last but not least, fitting the training data to a model. Enjoy!\n\nInstructions\n\nCreate diabetes_split, a split where the training set contains three-quarters of all diabetes rows and where training and test sets have a similar distribution in the outcome variable.\n\n\nset.seed(9)\n# Create the balanced data split\ndiabetes_split &lt;- initial_split(diabetes, \n                                prop = 0.75, \n                                strata = outcome)\n\n\nBuild a decision tree specification for your model using the rpart engine and save it as tree_spec.\n\n\n# Build the specification of the model\ntree_spec &lt;- decision_tree() |&gt;  \n  set_engine(\"rpart\") |&gt;  \n  set_mode(\"classification\")\n\n\nFit a model model_trained using the training data of diabetes_split with outcome as the target variable and bmi and skin_thickness as the predictors.\n\n\n# Train the model\nmodel_trained &lt;- tree_spec |&gt; \n  fit(outcome ~ bmi + skin_thickness, \n      data = training(diabetes_split))\n\nmodel_trained\n\nparsnip model object\n\nn= 576 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 576 201 no (0.6510417 0.3489583)  \n    2) bmi&lt; 29.85 213  34 no (0.8403756 0.1596244) *\n    3) bmi&gt;=29.85 363 167 no (0.5399449 0.4600551)  \n      6) bmi&lt; 40.85 297 126 no (0.5757576 0.4242424)  \n       12) bmi&gt;=40.05 10   1 no (0.9000000 0.1000000) *\n       13) bmi&lt; 40.05 287 125 no (0.5644599 0.4355401)  \n         26) skin_thickness&gt;=5.5 208  82 no (0.6057692 0.3942308)  \n           52) skin_thickness&lt; 25.5 48  11 no (0.7708333 0.2291667) *\n           53) skin_thickness&gt;=25.5 160  71 no (0.5562500 0.4437500)  \n            106) bmi&gt;=38.8 19   4 no (0.7894737 0.2105263) *\n            107) bmi&lt; 38.8 141  67 no (0.5248227 0.4751773)  \n              214) bmi&lt; 30.85 15   3 no (0.8000000 0.2000000) *\n              215) bmi&gt;=30.85 126  62 yes (0.4920635 0.5079365)  \n                430) skin_thickness&gt;=35.5 51  21 no (0.5882353 0.4117647) *\n                431) skin_thickness&lt; 35.5 75  32 yes (0.4266667 0.5733333)  \n                  862) skin_thickness&lt; 29.5 37  17 no (0.5405405 0.4594595) *\n                  863) skin_thickness&gt;=29.5 38  12 yes (0.3157895 0.6842105) *\n         27) skin_thickness&lt; 5.5 79  36 yes (0.4556962 0.5443038)  \n           54) bmi&lt; 37.65 66  32 no (0.5151515 0.4848485)  \n            108) bmi&gt;=33.05 30  10 no (0.6666667 0.3333333) *\n            109) bmi&lt; 33.05 36  14 yes (0.3888889 0.6111111) *\n           55) bmi&gt;=37.65 13   2 yes (0.1538462 0.8461538) *\n      7) bmi&gt;=40.85 66  25 yes (0.3787879 0.6212121)  \n       14) skin_thickness&gt;=37.5 24  11 no (0.5416667 0.4583333)  \n         28) skin_thickness&lt; 41.5 11   2 no (0.8181818 0.1818182) *\n         29) skin_thickness&gt;=41.5 13   4 yes (0.3076923 0.6923077) *\n       15) skin_thickness&lt; 37.5 42  12 yes (0.2857143 0.7142857) *\n\n# Graph the model\nrpart.plot::rpart.plot(model_trained$fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThat pipeline was perfectly handcrafted! Did you see that, according to the nodes, skin thickness lower than 5.5 and BMI value between 37.65 and 40.05 corresponds to a very high risk of diabetes? Let’s head over to the engine room to check your model’s performance."
  },
  {
    "objectID": "01-MLTB.html#predict-and-evaluate---video",
    "href": "01-MLTB.html#predict-and-evaluate---video",
    "title": "1  Classification Trees",
    "section": "Predict and evaluate - (video)",
    "text": "Predict and evaluate - (video)"
  },
  {
    "objectID": "01-MLTB.html#make-predictions",
    "href": "01-MLTB.html#make-predictions",
    "title": "1  Classification Trees",
    "section": "1.7 Make predictions",
    "text": "1.7 Make predictions\nMaking predictions with data is one of the fundamental goals of machine learning. Now that you know how to split the data and fit a model, it’s time to make predictions about unseen samples with your models.\nYou are going to make predictions about your test set using a model obtained by fitting the training data to a tree specification.\nAvailable in your workspace are the datasets that you generated previously (diabetes_train and diabetes_test) and a decision tree specification tree_spec, which was generated using the following code:\n\ntree_spec &lt;- decision_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"classification\") \n\n\nInstructions\n\nFit your specification to the training data using outcome as the target variable and all predictors to create model.\n\n\n# Train your model\nmodel &lt;- tree_spec |&gt;  \n  fit(outcome ~ ., data = diabetes_train)\n\n\nUse your model to predict the outcome of diabetes for every observation in the test set and assign the result to predictions.\n\n\n# Generate predictions\npredictions &lt;- predict(model, \n                       new_data = diabetes_test, \n                       type = \"class\")\n\n\nAdd the true test set outcome to predictions as a column named true_class and save the result as predictions_combined.\n\n\n# Add the true outcomes\npredictions_combined &lt;- predictions |&gt;  \n  mutate(true_class = diabetes_test$outcome)\n\n\nUse the head() function to print the first rows of the result.\n\n\n# Print the first  6 lines of the result\npredictions_combined |&gt; \n  head() |&gt; \n  kable()\n\n\n\n\n.pred_class\ntrue_class\n\n\n\n\nyes\nyes\n\n\nyes\nyes\n\n\nyes\nyes\n\n\nyes\nyes\n\n\nno\nno\n\n\nyes\nyes\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNow every predicted .pred_class has its true_class counterpart. The natural next step would be to compare these two and see how many are correct. You are about to find out in the next exercise."
  },
  {
    "objectID": "01-MLTB.html#crack-the-matrix",
    "href": "01-MLTB.html#crack-the-matrix",
    "title": "1  Classification Trees",
    "section": "1.8 Crack the matrix",
    "text": "1.8 Crack the matrix\nVisual representations are a great and intuitive way to assess results. One way to visualize and assess the performance of your model is by using a confusion matrix. In this exercise, you will create the confusion matrix of your predicted values to see in which cases it performs well and in which cases it doesn’t.\nThe result of the previous exercise, predictions_combined, is still loaded.\n\nInstructions\n\nCalculate the confusion matrix of the predictions_combined tibble and save it as diabetes_matrix. Print the result to the console.\n\n\n# The confusion matrix\ndiabetes_matrix &lt;- conf_mat(predictions_combined,\n                            truth = true_class,\n                            estimate = .pred_class)\n\n# Print the matrix\ndiabetes_matrix\n\n          Truth\nPrediction no yes\n       no  79  20\n       yes 23  32\n\n\n\nOut of all true no outcomes, what percent did your model correctly predict?\n\n\n79/(79 + 23)\n\n[1] 0.7745098\n\nsens(predictions_combined, \n     truth = true_class, \n     estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.775\n\n\n\n\n\n\n\n\nNote\n\n\n\nYour model found 77.5% of all positive (no diabetes) outcomes. This measure is called sensitivity."
  },
  {
    "objectID": "01-MLTB.html#are-you-predicting-correctly",
    "href": "01-MLTB.html#are-you-predicting-correctly",
    "title": "1  Classification Trees",
    "section": "1.9 Are you predicting correctly?",
    "text": "1.9 Are you predicting correctly?\nYour model should be as good as possible, right? One way you can assess this is by counting how often it predicted the correct classes compared to the total number of predictions it made. As discussed in the video, we call this performance measure accuracy. You can either calculate this manually or by using a handy shortcut. Both obtain the same result.\nThe confusion matrix diabetes_matrix and the tibble predictions_combined are loaded.\n\nInstructions\n\nPrint diabetes_matrix to the console and use its entries to directly calculate correct_predictions, the number of correct predictions. Save the total number of predictions to all_predictions. Calculate and the accuracy, save it to acc_manual, and print it.\n\n\ndiabetes_matrix\n\n          Truth\nPrediction no yes\n       no  79  20\n       yes 23  32\n\n# Calculate the number of correctly predicted classes\ncorrect_predictions &lt;- 79 + 32\n\n# Calculate the number of all predicted classes\nall_predictions &lt;- 79 + 20 + 23 + 32\n\n# Calculate and print the accuracy\nacc_manual &lt;- correct_predictions / all_predictions\nacc_manual\n\n[1] 0.7207792\n\n\n\nCalculate the accuracy using a yardstick function and store the result in acc_auto. Print the accuracy estimate.\n\n\n# The accuracy calculated by a function\nacc_auto &lt;- accuracy(predictions_combined,\n                     truth = true_class,\n                     estimate = .pred_class)\nacc_auto$.estimate\n\n[1] 0.7207792\n\n\n\nAccuracy is very intuitive but also has its limitations. Imagine we have a naive model that always predicts no, regardless of the input. What would the accuracy be for that model?\n\n\nHDF &lt;- data.frame(true_class = diabetes_test$outcome, \n                  .pred_class = factor(rep(\"no\", 154), \n                                       levels = c(\"no\", \"yes\")))\n\naccuracy(HDF, truth = true_class, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.662\n\n\nFor the naive model, it would be accurate 66.23% of the time.\n\n\n\n\n\n\nNote\n\n\n\nA naive model always predicting no is almost as good as our model. Luckily there are more useful performance metrics which we’ll cover later in the course. Stay tuned for Chapter 3!"
  }
]